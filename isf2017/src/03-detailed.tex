\section{Detailed Description of the Research Plan}

The master plan is to create and perpetuate a ``synthesis assistant'',
which is to programming what proof assistants are to logical reasoning.
Since programs are typically larger than a common mathematical proof,
our assistant will be required to push the limits on the automation
front.
Fortunately, most software comprises of many mundane, unsophisticated
details around a relatively small \emph{core insight}.
That is, there is a lot of repetition and room for reuse.
By designing generic, reusable components the assistant should be able
to handle the boring parts automatically, minimizing hand-holding by
the user, allowing them to focus on the insightful part.

\subsection{Rewrite-based Deductive Synthesis}

% I am writing about deductive synthesis in general, these first few
% sentences may belong further up.
Synthesis is the process of reading a program specification and
producing program code.
Deduction is a formal method that imitates human reasoning by defining
rules and the conditions in which they can be applied to draw new facts
from existing facts.
Hence deductive synthesis makes for a very natural combination, applied
to automated programming.
In this setting, the specification should consist of some formal description
of the input-output relation, plus additional concerns such as safety or
resource bounds.
Rules tell the reasoning engine what pieces of code can be used to implement
what specifications.
As is the case with automatic theorem provers, deductive reasoning is
computationally hard because there are may ways to apply deduction rules,
and this number keeps growing as more facts are deduced.
Programming by itself is a complex reasoning task, which requires planning,
abstraction, and decomposition.

The rewrite-based approach proposes an intuitive class of reasoning techniques
via substitution.
The concept of \emph{equals-for-equals} is so basic and commonly used in math
that sometimes we do it without even noticing.
In this project, we will build a rewrite engine that will build an engine
capable of rewriting program terms in an extended programming language that
can express both our specifications and executable programs.
Its task would be to rewrite specifications until they ``morph'' into an
acceptable program, acceptance defined by restricting the allowed subset of the
vocabulary and/or semantic restriction such as computational complexity
of the resulting program.
We will tackle the computational complexity problem by utilizing Program
Equivalence Graph, a data structure capable of compactly representing an
exponentially large set of equivalent terms;
since rewrites (generally) preserve equivalence,
the outcomes of a conceptually unbounded number of rewrite steps.

\subsection{Developing a Library of Rules and Tactics}

Once the core infrastructure is in place, we should focus on developing
a collection of useful rewrite rules and routines to guide their application.
While rules define classes of equivalent programs, tactics define desirable
properties of (intermediate) program terms within a class.
We call them ``tactics'' because they are reminiscent of tactics in proof
assistants (Coq, Isabelle), where they are used to carry out
small-ish proof steps such as induction, case analysis, bijection, etc.
They are what the user ultimately sees and invokes.

Concrete use cases that we already have in mind for tactics are:

\begin{itemize}
  \item Restriction of access to a value to a specific API or a proxy;
    e.g. replacing direct array accesses with a stack ADT (thus only allowing push and pop)
    or allowing to read the sum of the array but not individual elements.
  \item Different kinds of recursion; the most notable example being
    tail recursion, because then existing compiler optimizations can be
    used to turn the code into a loop.
  \item Finite differencing; instrumenting code for memoizing intermediate
    values and then updating them when operands change, generalizing results
    from SETL (\cite{TOPLAS1982/Paige,ESOP1990/Paige}).
    Finite differencing has a history beyond computer science, from when
    humans were performing a lot of calculations manually.
    Today, it plays a major role in some compiler optimizations
    (\cite{CACM1997/Cocke})
    and in algorithm design methodology (\cite{Book1997/Dijkstra,VHLL1974/Earley}).
\end{itemize}

\subsection{Theory of Saturating Rewrite Systems}

In the most general case, it is easy to see that exhaustive rewriting via
pattern-based rules is prone to divergence.
Directed rewrite systems have been studied in the past in which a well-formed
measure is used to prove the absence of infinite derivations --- in a similar
way to termination proofs for programs.
These techniques do not apply to our situation where bidirectional rules exist;
however, it is easy to see that bidirectionality by itself does not pose a
problem because rewriting in one direction and then the other yields the
original term, which has already been encountered and is therefore discarded.
Divergence of PEG rewriting is caused by rules that generate infinitely many
\emph{new} terms, for example $f(x) \to f(g(x))$.
Previous experience shows that careful construction of appropriate rules can
prevent divergence and still produce useful results~\cite{LPAR2013/Itzhaky}.
We call such systems \emph{saturating}, and wish to study their properties.
In particular, we are interested in:
\begin{itemize}
  \item Classes of rules that guarantee saturation;
  \item Effective detection of matching loops that cause non-saturation;
  \item Identifying a ``non-saturating core'', that is, a subset of the
    rules that cause divergence. 
\end{itemize}

These results would have practical implications on the design of
deductive synthesis engines.
They can help shape guidelines for the selection of rewrite rules for
tactical purposes according to synthesis goal.
We can also artificially prevent matching loops by bounding the number of
applications of rules from the non-saturating core.

On top of that, there is a long history surrounding treatment of
operators with known algebraic properties, such as commutativity
and associativity.
These can be encoded as equivalences, but this leads to an exponential
number of variations for terms, often making problems intractable when
they contain long chains.
This problem was deemed hard for symbolic theorem provers, but we
believe we can achieve good performance by natively wiring such treatment
into the internal representation of PEGs.

\subsection{When to Stop?}

As discussed so far, application of tactics is at the discretion of the
user, as is the decision to stop applying them and send the program to
the compiler.
The user has to be ``satisfied'' with the revisions made interactively.
Can this be automated as well?
In a more general setting, the user will set some kind of goals for the
resulting artifact to meet, and a bag of tactics that can be useful for
obtaining it.
The system will then have more freedom in applying tactics, but it also
means it will have to make more complex decisions.
In particular, it will have to decide when it is ``done''.

We have several propositions of metrics that can guide a system into
making such a decision.

\begin{paragraph}{Metrics for safety.} 
Theoretical work in logic~\cite{MLCS1994/Avron} suggests syntax-directed
criteria for classifying computable relations defined by formulas,
using the logical concept of \emph{domain irrelevance}.
This work has been applied to queries in database theory.
By adjusting the basic inference rules, a spectrum of safety predicates can
be defined, so this approach may apply to different domains.
\end{paragraph}

\begin{paragraph}{Dynamic rippling.}
In the area of automated reasoning and the design of proof assistants,
there have been several advances in discovering proofs by induction.
One of these techniques is called \emph{dynamic rippling}~\cite{Festschrift2017/JohanssonDB10}:
it quantifies the distance between a given hypothesis and a desired conjecture
(typically induction hypothesis and step) and attempts to carry out proof
steps that will decrease this measure.
This approach has been implemented in Isabelle/HOL and was included in
its inventor's doctoral dissertation.
\end{paragraph}

\begin{paragraph}{Resource-aware types.}
It is no shame to say that many program transformations targeted in this
proposal are high-level variants of compiler optimizations by their widely
accepted notion.
If so, a straight-forward metric would be an estimation of resources
that a program term will consume during its execution.
Such formalism exists in the form of type systems that encode resource
usage properties~\cite{CAV2012/Hoffman} such as time and space complexity.
While naturally providing some sound approximation rather than a precise
identification, it can guide the system to stop when the desired level
of performance has been obtained.
The experience of Prof. Nadia Polikarpova (listed as a collaborator) with such types, 
and her specialty, will surely prove useful for this project.
\end{paragraph}

\bigskip
The project will involve applying these metrics with appropriate 
instantiation of their components, and evaluating their effectiveness
in filtering candidate programs in a range of scenarios.

\subsection{Termination Proofs}

The projects described in the previous subsections all revolve around partial
correctness, that is, convergence of semantics on terminating executions.
This is common practice because
(i) it simplifies reasoning and makes proofs more compact and elegant;
(ii) most programs encountered terminate for very simple reasons.
Still, from a theoretical point of view, non-careful substitution of a subprogram
with a non-terminating expression is a known source for unsoundness.
To make the refinement process airtight, we will seek methods for generating,
alongside the development, an accompanying termination proof.
Two research questions stand out at this point:

\begin{enumerate}
  \item Is it enough to prove termination of the high-level program?
    While certainly not true in general, there are cases where this holds
    obviously. If the high-level program has a loop structure, and it terminates,
    then there is a well-founded measure that decreases by every iteration.
    If refinement preserves the loop structure and each step terminates trivially
    (e.g.  is loop-free) then the refined program must persist the same
    decrease in that measure, and therefore termintates.
  \item Can we adapt high-level termination proof to low-level?
    In cases where a program's termination does not imply its revision's,
    we still may be able to transform the termination proof along the same
    lines that transformed the program itself.
    A simple example is a \textit{gcd} computation algorithm due to Euclid.
    the original formulation uses subtraction as the basic step;
    a more efficient version may use division remainder and run for fewer
    iterations.
    Still, a single remainder operation is the equivalent of several subtraction
    operations, hence the same argument that was used for termination of
    Euclid's original version can be applied, with small changes, to the new variant.
\end{enumerate}
