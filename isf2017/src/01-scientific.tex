\section{Scientific Background}

It is definitely the Age of Software.
After having infiltrated our homes, our cars, our pockets and wrists,
after startup engineers have seemingly launched every app imaginable,
the demand for software refuses to decline.
Its success in the private space catalyzes its application to the public
space as well: clinics and offices shift toward Web-based appointment
scheduling and information delivery, and even government agencies are
inclined to offer their services online.
The thirst for information causes more data stores to open up to the
public, and the flow of data requires organization, searching and sorting
to make sense of it.
Soon enough our wallets will be filled with electronic currency and
manage trade contracts for us.
In this new economy it will become hard to imagine how we could ever
shop for groceries without software.

To cope with ever-increasing need to create more software, developers
will have to adopt new practices.
Today's development tools limit code reuse and collaboration in larger
scales, e.g. in an open source community setting.
Software modules are being rewritten and old code ages quickly and becomes
unusable due to rapidly changing APIs unless it is routinely maintained,
incurring high overheads.
Over time, code bases become harder to understand, maintain, and reuse.
Accompanying design documents and other documentation are written, but
they are hard to keep up-to-date with the code, and at any rate, it does
not seem that the right way to understand a computer program is to read
a book about it.

\begin{paragraph}{Clear separation of semantics and implementation.}
Most software starts its life as clean, elegant code, thoughtfully designed.
What happens then is that concerns such as portability, performance,
and integration with other software arise, and the developers are forced
to revise their code.
Code that has been subject to many changes is notorious for becoming
obfuscated, and also starts to deviate from the original design and
documentation.
A particular syndrome we wish to address is that the \emph{meaning} of
the software, often called its \emph{semantics}, sometimes \emph{formal specification},
and colloquially \emph{"The What"}, is intertwined and mixed with all
other aspects of programming within the code --- its \emph{implementation},
\emph{"The How"}.
Creating software layers for software semantics and its implementation,
in conjunction with a corresponding compiler that knows about the connection
between the two, can help manage the risks from changes by (i) only changing the
specification when requirements actually change, (ii) keeping the specification
smaller than the whole program, making it easier to review, and (iii) making
sure, mechanically, that the correlation between the layers is maintained.
\end{paragraph}


\begin{paragraph}{Not repeating yourself.}
The previous point is very much addressed by software engineering approaches
like design-by-contract and by formal methods such as model checking and Hoare-Floyd
style verification.
What these techniques generally lack is the transference of information across
abstraction layers:
after writing a contract, a programmer has to provide an implementation,
which repeats many of the details of the contract in code form;
after writing a procedure, to verify it, a programmer has to provide a formal
specification in some logic, repeating many of the implementation details.
To avoid repetitiveness, some form of synthesis is required, backed up by some
form of formal reasoning that tells the system which parts can be re-used when
refining and which parts change and need to be specified.
\end{paragraph}

\begin{paragraph}{Related Work}
Work on semi-automated programming began quite long ago at Kestrel Institute~\cite{AI1985/Smith},
which led to the development of several prototype projects,
KIDS~\cite{TSE1990/Smith}, DTRE~\cite{CPS1991/Blaine}, 
and Specware~\cite{2001/McDonald:specware}.
These systems are built on the concept of an algorithmic knowledge base encoded
as a library, and matched against program specifications to apply the
right transformation and obtain a correct-by-construction implementation.
KIDS contains one of the early interactive proof assistants, to support
fulfillment of pre-conditions required for some algorithmic solutions.

AutoBayes~\cite{JFP2003/Fischer} is an example for a domain-specific automatic programming system
developed by NASA.
It contains a large database of statistical algorithms for constructing and
analyzing models.
A solver tries different \emph{schemas}, allowing backtracking to find
a computationally optimal combination, and derives a specialized algorithm
for learning a specific model supplied by the user.
\end{paragraph}
