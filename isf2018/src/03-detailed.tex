\section{Detailed Description of the Research Plan}

The master plan is to create and perpetuate a ``synthesis assistant'',
which is to programming what proof assistants are to logical reasoning.
Since programs are typically larger than a common mathematical proof,
our assistant will be required to push the limits on the automation
front.
Fortunately, most software comprises of many mundane, unsophisticated
details around a relatively small \emph{core insight}.
That is, there is a lot of repetition and room for reuse.
By designing generic, reusable components the assistant should be able
to handle the boring parts automatically, minimizing hand-holding by
the user, allowing them to focus on the insightful part.

\subsection{Rewrite-based Deductive Synthesis}

% I am writing about deductive synthesis in general, these first few
% sentences may belong further up.
Synthesis is the process of reading a program specification and
producing program code.
Deduction is a formal method that imitates human reasoning by defining
rules and the conditions in which they can be applied to draw new facts
from existing facts.
Hence deductive synthesis makes for a very natural combination, applied
to automated programming.
In this setting, the specification should consist of some formal description
of the input-output relation, plus additional concerns such as safety or
resource bounds.
Rules tell the reasoning engine what pieces of code can be used to implement
what specifications.
As is the case with automatic theorem provers, deductive reasoning is
computationally hard because there are may ways to apply deduction rules,
and this number keeps growing as more facts are deduced.
Programming by itself is a complex reasoning task, which requires planning,
abstraction, and decomposition.

The rewrite-based approach proposes an intuitive class of reasoning techniques
via substitution.
The concept of \emph{equals-for-equals} is so basic and commonly used in math
that sometimes we do it without even noticing.
In this project, we will build a rewrite engine that will be
capable of finding equivalent substitutes for program terms in an extended programming language that
can express both our specifications and executable programs.
Its task would be to rewrite specifications until they ``morph'' into an
acceptable program, acceptance defined by restricting the allowed subset of the
vocabulary and/or enforcing semantic restriction such as computational complexity
of the resulting program.
We will tackle the computational complexity problem by utilizing Program
Expression Graphs~\cite{POPL2009/Tate} (used \eg in~\cite{PLDI2015/Panchekha}),
a data structure capable of compactly representing an exponentially large
set of equivalent terms;
since rewrites (generally) preserve equivalence,
the outcomes of a conceptually unbounded number of rewrite steps
can be succinctly stored using a single PEG.

The project will focus on manipulating PEGs and traversing them, finding
efficient ways to search for terms and extract them from the data structure,
and controlling the order in which terms are generated --- for example,
short terms first, which is a common approach alluding to Occam's razor
(approximating that short programs are simpler than longer ones).
We will design a language for interacting with the engine, and construct 
example programs based on real-world needs to demonstrate its capabilities.


\subsection{Developing a Library of Rules and Tactics}

Once the core infrastructure is in place, we should focus on developing
a collection of useful rewrite rules and routines to guide their application.
While rules define classes of equivalent programs, tactics define desirable
properties of (intermediate) program terms within a class.
We call them ``tactics'' because they are reminiscent of building
blocks of the same name present in proof
assistants (Coq, Isabelle), where they are used to carry out
small-ish proof steps such as induction, case analysis, bijection, etc.
They are what the user ultimately sees and invokes.

Concrete use cases that we already have in mind for tactics are:

\begin{itemize}
  \item Restriction of access to a value to a specific API or a proxy;
    e.g. replacing direct array accesses with a stack ADT (thus only allowing push and pop)
    or allowing to read the sum of the array but not individual elements.
  \item Different kinds of recursion; the most notable example being
    tail recursion, because then existing compiler optimizations can be
    used to turn the code into a loop.
  \item Finite differencing; instrumenting code for memoizing intermediate
    values and then updating them when operands change, generalizing results
    from SETL (\cite{TOPLAS1982/Paige,ESOP1990/Paige}).
    Finite differencing has a history beyond computer science, from when
    humans were performing a lot of calculations manually.
    Today, it plays a major role in some compiler optimizations
    (\cite{CACM1997/Cocke})
    and in algorithm design methodology (\cite{Book1997/Dijkstra,VHLL1974/Earley}).
\end{itemize}

The last point can be illustrated with an example: a \emph{prefix sum} of a list
is defined as a list containing the sums of all (non-empty) prefixes of the
first list.
It can be written succinctly as (the syntax is based loosely on Haskell)
\begin{center}
\textsf{prefix-sum $\ell$ = map ($\lambda$i. sum (take i $\ell$)) (1..$|\ell|$)}
\end{center}

This version conveys the meaning precisely but ignores performance, leading to
quadratic running time, since the \textsf{sum} will be forced to run every prefix
individually.
We know, however, that the \textsf{i+1}'th prefix can be obtained from the \textsf{i}'th
prefix by a single addition of the \textsf{i+1}'th element.
Consider how several steps of reasoning are required of the programmer to reach such
conclusion: that \textsf{take (i+1)} differs from \textsf{take i} by just one element;
that \textsf{sum} can be split at any point in the input list due to the associativity
of addition; and that \textsf{sum (take i $\ell$)} is itself available since it has
just been computed.
Our first challenge will be to display that such reasoning can be done automatically.


\subsection{High-performance Application Domains}

The core of this project would be the application of such ideas in the context of
algorithmic implementations.
In that aspect, it strives to put into a formal framework the development of such
implementations, one that is currently done manually and informally.

\begin{paragraph}{Parallelization}
Parallelizing compilers are not quite able to achieve high-performing code, especially
with today's complex hardware.
In their seminal work, Chowdhury el al.~\cite{SODA2006/Chowdhury} have shown how the
recursive divide-and-conquer approach transcends loop parallelization in the existence
of memory hierarchy (specifically, hardware caches).
My work on Bellmania~\cite{OOPSLA2016/Itzhaky} was devoted to putting this method on
more solid, formal grounds by encoding the conditions in which transformations can
be applied safely into logical guards that are subsequently checked with an SMT solver.
In this project, we will pursue other recent techniques from the algorithms community,
such as listed in~\cite{Thesis/Shun,SPAA2018/Dhulipala}, and strive to encode the
reasoning behind them in such manner.
As the first examples to consider, we have a parallel version of the prefix sum~\cite{1993/Blelloch}
and a parallel DFS~\cite{SC2015/Acar}, which will open the way for more graph
algorithms.
\end{paragraph}

\begin{paragraph}{Stream Processing}
Processing of high-capacity, high-speed data streams is ubiquitous in analytics~\cite{ECOOP2014/Vaziri}.
For example, a recommendation system for Amazon may be required to deal with a continuous
stream of transactions, maintaining statistical figures that will direct future outputs.
It is usually not desirable, or even possible, to store the entire history.
It may, however, be most natural to represent the functionality as a computation on that history.
From that, a derivation system will be able to assist the programmer to obtain an implementation
that does not access history too far in the past, by continuously updating exiting aggregates.
The use of differencing, mentioned above, will be central to this purpose.
\end{paragraph}

\begin{paragraph}{Zero-copy Implementations}
The need to copy data in memory is known to be a source of performance
degradation in medium to large system, such as those handling communication
protocols, HTML-based Web frontends and other GUIs, data management software,
and also low-level device drivers.
Typically data has to be buffered, in some intermediate data structure,
before it can be passed to the next processing routine.
This is done for reasons of software engineering: the module that processes
the data should not be exposed e.g. to the communication layer primitives,
so the two modules have to communicate by passing some data between
them.
The simplest way to achieve that would be to store the data in memory
and exchange memory addresses.
This, however, means that data is assembled in memory only to be promptly
disassembled (e.g. parsed, or split and sent to a database), resulting in
memory consumption and slowdown.
Occasionally, to connect two existing software components, the data
has to be converted from one data structure to another (e.g., one module
produces a tree, while a second expects a hash table), so an additional
copying phase is required.
Using the synthesis and refinement techniques that we develop, a programmer
will be able to refer to those memory buffers as ephemeral, that is, used only
as a means to describe the semantics of the computation,
and use the synthesizing compiler to fuse the implementation code at data
transfer points, eliminating unnecessary copying.
\end{paragraph}

\subsection{Theory of Saturating Rewrite Systems}

In the most general case, it is easy to see that exhaustive rewriting via
pattern-based rules is prone to divergence.
Directed rewrite systems have been studied in the past in which a well-formed
measure is used to prove the absence of infinite derivations --- in a similar
way to termination proofs for programs.
These techniques do not apply to our situation where bidirectional rules exist;
however, it is easy to see that bidirectionality by itself does not pose a
problem because rewriting in one direction and then the other yields the
original term, which has already been encountered and is therefore discarded.
Divergence of PEG rewriting is caused by rules that generate infinitely many
\emph{new} terms, for example $f(x) \to f(g(x))$.
Previous experience shows that careful construction of appropriate rules can
prevent divergence and still produce useful results~\cite{LPAR2013/Itzhaky}.
We call such systems \emph{saturating}, and wish to study their properties.
In particular, we are interested in:
\begin{itemize}
  \item Classes of rules that guarantee saturation;
  \item Effective detection of matching loops that cause non-saturation;
  \item Identifying a ``non-saturating core'', that is, a subset of the
    rules that cause divergence. 
\end{itemize}

These results would have practical implications on the design of
deductive synthesis engines.
They can help shape guidelines for the selection of rewrite rules for
tactical purposes according to synthesis goal.
We can also artificially prevent matching loops by bounding the number of
applications of rules from the non-saturating core.

On top of that, there is a long history surrounding treatment of
operators with known algebraic properties, such as commutativity
and associativity.
These can be encoded as equivalences, but doing this leads to an exponential
number of variations for terms, often making problems intractable when
they contain long chains.
This problem was deemed hard for symbolic theorem provers, but we
believe we can achieve good performance by natively wiring such treatment
into the internal representation of PEGs.

\subsection{When to Stop?}

As discussed so far, application of tactics is at the discretion of the
user, as is the decision to stop applying them and send the program to
the compiler.
The user has to review the results and become ``satisfied'' with the revisions made interactively.
Can this be automated as well?
In a more general setting, the user will set some kind of goals for the
resulting artifact to meet, and a bag of tactics that can be useful for
obtaining it.
The system will then have more freedom in applying tactics, but it also
means it will have to make more complex decisions.
In particular, it will have to decide when its task is ``done''.

We have several propositions of metrics that can guide a system into
making such a decision.

\begin{paragraph}{Metrics for safety.} 
Theoretical work in logic~\cite{MLCS1994/Avron} suggests syntax-directed
criteria for classifying computable relations defined by formulas,
using the logical concept of \emph{domain irrelevance}.
This work has been applied to queries in database theory.
By adjusting the basic inference rules, a spectrum of safety predicates can
be defined, so this approach may apply to different domains.
\end{paragraph}

\begin{paragraph}{Dynamic rippling.}
In the area of automated reasoning and the design of proof assistants,
there have been several advances in discovering proofs by induction.
One of these techniques is called \emph{dynamic rippling}~\cite{Festschrift2017/JohanssonDB10}:
it quantifies the distance between a given hypothesis and a desired conjecture
(typically induction hypothesis and step) and attempts to carry out proof
steps that will decrease this measure.
This approach has been implemented in Isabelle/HOL and was included in
its inventor's doctoral dissertation.~\cite{Thesis2009/Johansson}.
Although rippling, in itself, is not sufficient to completely automate
inductive proofs, its efficacy in mechanizing part of the reasoning
within an interactive setting has been demonstrated by its inclusion
in Isabelle/HOL.
\end{paragraph}

\begin{paragraph}{Resource-aware types.}
It is no shame to say that many program transformations targeted in this
proposal are high-level variants of compiler optimizations according to their widely
accepted notion.
If so, a straight-forward metric would be an estimation of resources
that a program term will consume during its execution.
Such formalism exists in the form of type systems that encode resource
usage properties~\cite{CAV2012/Hoffman} such as time and space complexity.
While naturally providing some sound approximation rather than a precise
identification, it can guide the system to stop when the desired level
of performance has been obtained.
We will benefit from the expertise of Prof. Nadia Polikarpova (listed as a collaborator) with such types, 
and her specialty in the area of deductive synthesis, in the design of a suitable type characterization for program derivation.
\end{paragraph}

\bigskip
The project will involve applying these metrics with appropriate 
instantiation of their components, and evaluating their effectiveness
in filtering candidate programs in a range of scenarios.

\subsection{Termination Proofs}

The projects described in the previous subsections all revolve around partial
correctness, that is, convergence of semantics on terminating executions.
This is common practice because
(i) it simplifies reasoning and makes proofs more compact and elegant;
(ii) most programs encountered terminate for very simple reasons.
Still, from a theoretical point of view, non-careful substitution of a subprogram
with a non-terminating expression is a known source for unsoundness.
To make the refinement process airtight, we will seek methods for generating,
alongside the development, an accompanying termination proof.
Two research questions stand out at this point:

\begin{enumerate}
  \item Is it enough to prove termination of the high-level program?
    While certainly not true in general, there are cases where this holds
    obviously. If the high-level program has a loop structure, and it terminates,
    then there is a well-founded measure that decreases by every iteration.
    If refinement preserves the loop structure and each step terminates trivially
    (e.g.  is loop-free) then the refined program must persist the same
    decrease in that measure, and therefore termintates.
  \item Can we adapt high-level termination proof to low-level?
    In cases where a program's termination does not imply its revision's,
    we still may be able to transform the termination proof along the same
    lines that transformed the program itself.
    A simple example is a \textit{gcd} computation algorithm due to Euclid.
    the original formulation uses subtraction as the basic step;
    a more efficient version may use division remainder and run for fewer
    iterations.
    Still, a single remainder operation is the equivalent of several subtraction
    operations, hence the same argument that was used for termination of
    Euclid's original version can be applied, with small changes, to the new variant.
\end{enumerate}
