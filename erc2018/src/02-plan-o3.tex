\subsection{Modular Reasoning with Lemmas}
\label{plan-modular}

Mathematical proofs are seldom monolithic.
Usually, there are some intermediate conjectures --- \emph{lemmas} --- that
are proved one by one, gradually building toward the culmination of the main
theorem.
Mainly, it helps the author overcome the complexity of the proof development
task: in a formal setting, even a seemingly benign logical step can have many
inference steps; and the steps are not always sequential, for example when
a case split is required in order to consider several scenarios.
A hierarchical organization of tasks helps the author make sure that all the bases are covered, while not drowning in details.
This style of writing proofs has the additional benefit of making a proof easier to understand for potential readers.

There is much in common between this practice of organizing proofs and sub-proofs, and proven practices in software engineering.
In fact, the term \emph{proof engineering} has been coined recently~\cite{FTPL2019:Ringer} to drive this precise idea.
Like a programmer working on large systems, a scientist  developing complex proofs constructs abstractions, embodied in mathematical definitions, and then reasons about these abstractions at a high level, ignoring some concrete underlying details.
The scientist will prove properties about their abstractions as \emph{ancillary lemmas}, and then use these lemmas to derive desired properties.
What we intend to do in this research is to anchor this practice in support tools, and increase the level of automation by \textbf{automatically discovering and proving} these ancillary lemmas.

\begin{proposal}
Effective \emph{automatic} proof engineering requires a bidirectional approach, combining ``top-down'' reasoning --- decomposing top-level goals into smaller pieces,
and ``bottom-up'' reasoning --- constructing new conjectures that are consequences of known facts.
\end{proposal}

The core of this idea is in itself not new.
Starting from early theorem provers, the concept of \emph{information retention}~\cite{JAR1995:Voronkov} --- the derivation of new clauses by applying known inference rules and rewriting with equalities --- has been identified as an important feature for automated theorem proving, \esp when included in a prover that follows a top-down search strategy.
However, this proposal offers a new perspective on this aspect:
\begin{itemize}
  \item Where existing methods build consequences using small-step inference rules, we propose a paradigm that will make \textbf{larger logical leaps} by building on recent advances in lemma synthesis~\cite{CADE2013:Claessen,ITP2017:Johansson,arXiv2020:Singher,CAV2021:Murali} to create more useful lemmas that will hit the goals faster.
  \item We rethink the concept of \textbf{bottom-up through the lens of cyclic proofs}.
  Because the new systems admit infinite proofs~\cite{LICS2007:Brotherston,CPP2017:Rowe,IJCAR2020:Cohen}, they do not necessarily have a ``bottom'' in the traditional sense.
\end{itemize}

We allude to incremental construction of loop invariants presented in~\cite{arXiv2020:Padon,FAC2008:Bradley} as an inspiration for new, exciting ways to combine top-down and bottom-up search.


\begin{researchquestion}
What is an effective measure of \emph{usefulness} for ancillary lemmas, for the purpose of guiding theory exploration?
\end{researchquestion}

In~\cite{arXiv2020:Singher}, we explored an enumerative-synthesis approach to theory exploration with an emphasis on lemmas whose proofs require the use of induction.
A simple heuristic was used to choose lemmas that will be as generally applicable as possible.
This was sufficient for theorem proving benchmarks, but as the number of function symbols in the vocabulary grows, a more aggressive pruning will be necessary to avoid an unmanageable blowup in the number of lemmas.
We will develop an \textbf{a-priori clustering} of symbols from the vocabulary that are more tightly coupled, so that theory exploration can be applied to smaller vocabularies.
The clustering will be driven by the system's experience of proving theorems, which is accumulated over time, using statistical inference
This would mean that the prover will become more biased, but potentially much more effective, the more proofs it manages to successfully generate.
Even though we do not plan to use a deep-learning approach, a form of curriculum learning, where the system is exercised over a collection of problems with increasing complexity, will be used to bootstrap the prover's knowledge.

\begin{researchquestion}
What guidance metric would a prover use in order to \emph{untangle} a complex proof goal that mixes several mathematical concepts?
\end{researchquestion}

In order to make use of our lemmas, the vocabularies, that is, sets of logical symbols, occurring in propositions has to be clustered into ``tightly-coupled'' concepts.
Intuitively, functions or relations are tightly coupled if they occur together in ancillary lemmas; the more lemmas combine them, the more tightly coupled should they be considered.
This has practical implications:
If the prover manages to split the proof goal into sub-goals such that each sub-goal consists of tightly-coupled symbols, it will then have a larger variety of relevant lemmas to make progress with.
This \emph{untangle phase} can be multi-layered, since to get a separation into clusters A, B and C a prover may have to first separate A from B+C, and then separate B from C.
Each such separation is rarely a single derivation in the proof system, so some search, and backtracking, may be needed to nail down the right decomposition.
Here, too, I will apply statistical bias based on previous successful attempts at decomposing problems,
mimicking to a large degree the process of training an experienced mathematician.