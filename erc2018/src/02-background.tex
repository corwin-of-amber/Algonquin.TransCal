\section{State of the art and objectives}

The correctness of computer-controlled systems is an open problem in computer science that, despite significant theoretical advances, does not have a satisfactory, practical solution.
Software has been growing steadily in volume and complexity since the 1970's, and compiler architecture grew along with it.
Automated verification is lagging behind: It has only become reasonably tractable in the early 2000's, at which time the size of software systems was already measured in millions of lines of code.
Since then, it is struggling to catch up with the software industry, while at the same time, technological advances increase our reliance on software as well as the complexity of computer systems.
Most of the quality assurance processes rely on intensive testing, and Symbolic Execution~\cite{CACM2013:Cadar} has become a valuable tool for automatic generation of tests for detecting bugs. 
However, such tools do not provide guarantees and cannot prove correctness properties of the underlying program.

All software verification tools follow a common pattern:
First, a frontend inspects the source code and constructs a semantic representation (some examples for this intermediate representation are Boogie~\cite{PASTE2005:Barnett}, Viper~\cite{VMCAI2016:Muller}, and LLVM IR).
Then, a backend derives a set of \emph{verification conditions} --- logical statements whose validity implies the correctness of the property being verified.
These logical statements are sent to a prover to establish their validity.
This proposal focuses on the latter phase, that of finding the proof of validity, where the input formula is a given.
Still, it is important to remember, that better provers influence the design choices in the upper layers, which may contribute to the overall success of the tool.

In this proposal, we focus on the two most predominant disciplines that have evolved to address the problem of automatically proving the validity of logical formulas, in the general setting of mechanizing the foundations of mathematics as well as for the more specific use for software verification.
Despite ongoing efforts in enhancing the theory and honing the implementations, neither direction has been able to produce tools that offer a comprehensive treatment of proof automation.
Portfolio approaches~\cite{ESOP2013:Filliatre} attempt to bind together a variety of provers, heuristically trying several tools on each problem instance in the hope that one of them will fit the bill ---
but eventually, these systems only amount to the sum of their parts.
At the same time, the emergence and increasing popularity of \emph{interactive theorem proving} teaches us that for realistic goals, even academic ones~\cite{DeepSpec}, the total coverage of existing tools is insufficient.
On the other hand, interactive theorem proving in proof assistants is laborious, requiring a large amount of training and manual effort.
I will describe the two main research disciplines that, at the time of writing, dominate the field of automated reasoning, survey the state of the art in each of them, and describe how a better paradigm can be obtained by \textbf{fusing the existing methodologies} under a unified theory and practice.

\vspace{5pt}\noindent
\begin{tabular}{p{8cm}p{8cm}}
\textbf{ATP: Automated Theorem Proving} &
\textbf{SMT: Satisfiability Modulo Theories} \\
Admittedly, the term is somewhat of a misnomer, since all the techniques surveyed here can be titled ``automated provers''.
However, the term ATP has become associated with a narrower flavor of theorem proving, based on \emph{superposition calculus} --- itself a term that bundles not only superposition (a technique for dealing with equalities and their implications) but several other concepts, such as resolution and saturation.
They are based on classical proof theory and fundamental concepts of syntactic substitution and term unification.
&
SMT solvers forked off the ``mainstream'' theorem proving crowd around the year 2000,
and are characterized by their heavy reliance on Boolean SAT decision procedures.
They have become specialized in certain logical fragments, mostly driven by the needs of hardware and software verifications, such as linear integer arithmetic, bit-vectors, and arrays.
Practically all of these are first-order.
Reasoning about property that combined multiple fragments is achieved by representing information that crosses theory boundaries as equalities.
\end{tabular}


\subsection{Recent advances in proof theory and ATP}

Automated theorem provers are based on the foundation of formal proof systems.
A proof system consists of a language of \emph{judgements} and a set of \emph{inference rules} that facilitate derivation of new judgements from existing ones.
The most notable proof systems today are a form of \emph{sequent calculus}, where judgements are \emph{sequents} of the form $\Gamma \vdash \Delta$ where $\Gamma,\Delta \subset \Lang$ are finite sets of formulas from a logic $\Lang$, which can be, for example, first-order logic over some vocabulary.
The semantics of $\Gamma\vdash\Delta$ is: for every structure $M$ and valuation $\sigma$, if $M,\sigma\models\varphi$ for every $\varphi\in\Gamma$, then there exists $\psi\in\Delta$ such that $M,\sigma\models\psi$.
The connection with the logical semantics is utilized in meta-proofs about the soundness and/or completeness of the proof system,
but when constructing proofs (manually or automatically) the semantics is largely ignored, and only the syntactic form of the judgement, as embodied in the system's inference rules,
is utilized for reasoning.

\begin{figure}
\centering
\renewcommand\arraystretch{1.2}
\[
\begin{array}{c@{\qquad}c}
\begin{array}{c}
~ \\ \hline
\Gamma \vdash \Delta, \RTC{x,y}{\varphi}{s,s}
\end{array}
\,\rulename{TC$_\textrm{refl}$}
%
&
\begin{array}{c}
\Gamma \vdash \Delta, \RTC{x,y}{\varphi}{s,r} \quad
\Gamma \vdash \Delta, \varphi[r,t]            \\ \hline
\Gamma \vdash \Delta, \RTC{x,y}{\varphi}{s,t}
\end{array}
\,\rulename{TC$_\textrm{R}$}
\\[5ex]
\multicolumn{2}{c}{
\begin{array}{c}
\Gamma, s = t \vdash \Delta \quad
\Gamma, \RTC{x,y}{\varphi}{s,z}, \varphi[z,t] \vdash \Delta \quad
z~\textrm{\small fresh}
\\ \hline
\Gamma,\RTC{x,y}{\varphi}{s,t}
	\vdash \Delta
\end{array}
\,\rulename{TC$_\textrm{L}$}
}
\end{array}
\]
({\small $\varphi[s,t]$ is a shorthand for $\varphi[s/x,t/y]$})
\caption{An example for a cyclic proof system for transitive closure.}
\label{b2:tc-cyclic}
\end{figure}

Inference rules take the form shown in \autoref{b2:tc-cyclic}: premises are judgements written above the line, and a conclusion is written below the line.
Each rule is, in fact, a \emph{schema}, since the names $\Gamma, \Delta, \varphi, s, t, x$ \etc are placeholders for formulas, terms, or variables, as appropriate.
A proof of a sequent $\Gamma\vdash\Delta$ is, at its core, a sequence $\Phi_1, \Phi_2, {\cdots}, \Phi_n$ of judgements such that each $\Phi_i$ is \emph{derived} from $\Phi_{r_1}, {\cdots}, \Phi_{r_k}$ ($r_{1..k} < i$) using one of the inference rules, and $\Phi_n = \Gamma\vdash\Delta$.
Commonly, $\Phi_{1..n}$ are portrayed in a directed tree where the direct children of $\Phi_i$ are the corresponding $\Phi_{r_{1..k}}$ from the derivation.
If some $\Phi_j$ is used as a premise in more than one derivation step,
there are two alternative ways to represent it:
Either duplicate $\Phi_j$ (and all of its descendants) when depicting the proof as a tree, or switch to a DAG representation where each judgement can have any number of parents.

\begin{paragraph}{Superposition theorem provers and saturation}
The term ``superposition'' is used to describe a family of theorem provers that are based on two main principles of deduction: \emph{resolution} and \emph{superposition}.
Both use the core concept of \emph{unification}.
Given two terms $t_1, t_2$, a \emph{unifier} is a (syntactic) substitution $\theta$ such that $t_1\theta = t_2\theta$.
The equality here designates syntactic identity, and $t\theta$ denotes applying a substitution to a term.
A classical result in logic tells us that if two terms have a unifier, then they also have a \emph{most general unifier} (\emph{mgu}):
A substitution $\theta_1$ is considered more general than another $\theta_2$ if the latter can be obtained from the former via sequential composition, $\theta_2 = \theta'\circ\theta_1$.
Following are a few select inference rules based on this concept.

\noindent\vspace{0pt}
\[
\begin{array}{c}
A \lor C_1 \quad \lnot A' \lor C_2 \\ \hline
(C_1 \lor C_2)\theta^{\scriptscriptstyle A}_{\scriptscriptstyle \!A'}
\end{array}
\,\rulename{RES}
\qquad
\begin{array}{c}
l=r \lor C_1 \quad L[s] \lor C_2 \\ \hline
(L[r] \lor C_1 \lor C_2)\theta^{s}_{l}
\end{array}
\,\rulename{SUP$^=_0$}
\qquad
\begin{array}{c}
l=r \lor C_1 \quad t[s]=t' \lor C_2 \\ \hline
(t[r]=t' \lor C_1 \lor C_2)\theta^{s}_{l}
\end{array}
\,\rulename{SUP$^=_1$}
\]

In the above, $\theta^{\alpha}_{\!\beta}$ is the mgu of $\alpha$ and $\beta$.
In addition to that, a superposition inference system makes use of a \emph{simplification order} $\succ$ on terms.
In \rulename{SUP$^=_0$} and \rulename{SUP$^=_1$},
application is restricted to cases where
$t'\theta \not\succeq t[s]\theta$.
This breaks the symmetry of equality and provides the prover with a direction.

Vampire~\cite{JAR1995:Voronkov,CAV2013:Kovacs} is the most reknown superposition theorem prover.
It works by \emph{saturation}: Vampire maintains a set $D$ of first-order clauses,
and incrementally grow it by selecting $\Phi_{r_{1..k}} \subseteq D$ and adding a clause $\Phi_{n+1}$ according to one of the inference rules.
Vampire terminates when it derives the empty clause ($\bot$, representing a contradiction),
or when $D$ is saturated, \ie, no new clauses can be added.

Another notable prover is E~\cite{AIC2002:Schulz}, a ``brainiac'' theorem prover:
It also uses superposition but invests more work in a careful selection of clauses and literals to operate on in each iteration.
This means a lower iteration rate, but allows the prover to focus on the more relevant parts of the proof.
The DISCOUNT proof search strategy makes not of ``active'' clauses and uses a search heuristic that favors them.
This heuristic can also be customized by the user of the tool, which is one of the main strengths of E.
\end{paragraph}

\begin{figure}
\input{img/fig-cyclic-proof}
\caption{An example for a cyclic proof with transitive closure.}
\label{b2:tc-cyclic-example}
\end{figure}

\begin{paragraph}{Cyclic proofs}
Traditionally, proofs are required to be finite, which is a sensible design choice, since the most important aspect of formal proofs is that they can be checked mechanically and validated by an algorithm.
However, more recently, a somewhat surprising player entered the field by challenging this restriction and allowing infinite proofs~\cite{LICS2007:Brotherston}.
Relaxing the finitary condition demands another, external condition in order to preserve the soundness of the proof system.
It is perhaps this \textbf{separation of concerns} that admits cyclic proofs that are more compact and elegant than traditional finitary proofs --- which is what brought them growing popularity.

In the cyclic framework, derivations are permitted to be regular,
non-well-founded (\ie,~infinitely tall) trees.
Regularity ensures that an infinite derivation tree always has a finite representation
as a (possibly) cyclic graph.
Concretely, these derivations are represented as finite trees, 
along with a set of \emph{backlinks} connecting each
non-terminal leaf node --- called \emph{bud} ---
to a syntactically identical ancestor node --- called \emph{companion}.
In other words, we allow goals from the middle of the proof to be
used again as premises higher up the derivation tree.
Formally, each backlink denotes the infinite unfolding of the
path connecting the bud with its associated companion.
We call such derivations \emph{pre-proofs}, since, alluding to the proofs-as-programs duality, they do not necessarily correpond to \emph{terminating} programs.
To ensure termination, we require that pre-proofs satisfy
an additional global property, defined in terms of \emph{traces} of syntactic elements, which can be formulas or terms occurring in sequents along a path in the proof tree.

To understand traces, observe the example derivation of \autoref{b2:tc-cyclic-example}, which uses the rules of \autoref{b2:tc-cyclic} along with rules for equality and a \emph{cut} rule (a sequent version of resolution).
A backlink (purple arrow) connects the bud (top right) with a companion (bottom), which are syntactically identical up to $\alpha$-renaming ($i\mapsto b$).
The tree path connecting the companion to the bud consists of five sequents, in which the traced elements are:
$a[n^*]b\leadsto a[n^*]i\leadsto a[n^*]i\leadsto a[n^*]i\leadsto a[n^*]i$.
Notice that cut and equality rules keep ``tracking'' the same element, and the destructuring rule {\rulename{TC$_\textrm{L}$}} pairs a TC formula $a[n^*]b$ with a TC formula representing a shorter path, $a[n^*]i$ ($i$ is a predecessor of $b$).
In this manner, the tracking structure of a cyclic system's inference rules draw a connection between syntactic entities and the semantic interpretations of them, such that \emph{some well-founded measure} (in the case of this example, the length of an $n$-path in the model) is weakly decreasing along the proof branch.
Moreover, at least one step (here, $a[n^*]b\leadsto a[n^*]i$) is \emph{progressing}, that is, \emph{strictly} decreasing.
\end{paragraph}


\subsection{Recent advances in SAT and SMT}

Interest in algorithms for solving Boolean satisfiability (SAT) began with the development of the DPLL method \cite{DPLL1,DPLL2}.
In its core it is very simple:
It operates on a propositional CNF formula (conjunction of clauses), and follows two main principles:
(a) the \emph{unit propagation rule}, whereby a clause with a single literal determines the truth value of the variable therein; and
(b) the \emph{pure literal elimination rule}, whereby a variable occurring either only positively or only negatively can be omitted.
Whenever the procedure assigns a value of \emph{true} to some literal, the entire clause containing it is discharged.
When a value of \emph{false} is assigned, the literal is removed from the clause.
This can lead to the rules (a) and (b) being enabled many times in the course of execution.
Of course, (a) and (b) alone are not enough to solve all instances, so some guess-and-backtrack is needed to handle the unassigned variables.
Which values to guess and how to backtrack in a way that maintains the most information is crucial for the performance of the solver.

Conflict-Driven Clause Learning (CDCL)~\cite{CDCL} is the gold standard of SAT solvers today.
CDCL improves over DPLL by using \emph{non chronological} backtracking, allowing the solver to undo several decisions at once, and preserving the information obtained from the failed branch in a \emph{conflict clause} that is added to the collection.
The conflict clause then guides future decisions for assigning truth values to variables.
CDCL solvers such as CaDiCaL~\cite{SAT2020:CaDiCaL} are being actively developed to this very day.
Techniques and heuristics for choosing the backtracking destination and constructing conflict clauses is an active research area.

\input{img/fig-smt}

Despite the complexity of SAT (it is NP-complete), today's solvers are capable of solving surprisingly large instances occurring in hardware verification.
For software verification, propositional logic is much too low-level to comfortably express our properties and semantics.
The research community was able to leverage the success of SAT solvers and extend it to satisfiability of first-order formulas, leading to Satisfiability Modulo Theories (SMT).
The ``theories'' refer to specific fragments of first-order logic for which efficient decision procedures have been constructed.
These theory solvers are then composed with the underlying SAT procedure to handle logical connectives, as illustrated in \autoref{b2:cdclt}:
A first-order formula is first reduced to a propositional one by replacing every atomic formula with a Boolean variable.
The result is then sent to the SAT component,
and if a satisfying (Boolean) assignment is found,
the theory solver is subsequently queried to test the feasibility of this assignment under the theory.
If it is not, then the theory solver emit a \emph{conflict clause}, which is appended to the formula, and the process repeats.
The combined method is referred to CDCL(T) (previously, DPLL(T)).
