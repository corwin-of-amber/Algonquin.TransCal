
\section{Quest for the Missing Link}

Beginning with the 1980's, the computer science community has accepted
that computer programs and logical proofs are made of the same fabric.
They are both based on a rigorous formal language, characterized by a grammar,
and they both rely heavily on tree-like structures for defining their semantics,
via syntax-directed derivations.
By now, it has become widely accepted that the task of writing a program can be
reduced to that of finding a proof to a conjecture, and conversely, the task of
proving a theorem can be reduced to producing a program with some specific
characteristics.
In addition to that, logic is at the core of software verification, as a basis
for specifications, loop invariants, rely-guarantee properties, and more.
The task of checking adherence to a program's specification is also reduced, in
more than one way, to deciding the validity of a logical formula.

This research is based on a fundamental understanding, that breaking existing
barriers in automated theorem proving is a key ingredient that will lead to
much more powerful software construction and certification mechanisms.
By powerful we mean both scalable to the sizes of realistic software modules
and requiring less user effort thanks to an increased level of automation.

Prominent reasoning techniques of our time are placed mostly in these two camps:

\paragraph{Theorem Provers (TP)} A family of tools and techniques based on proof
theory of logical deduction.
A logic in this setting is characterized by a vocabulary and a system of
inference rules; provers are tasked with searching the space of proofs, which
are essentially trees (more generally, DAGs) constructed from repeated
application of the inference rules.
The expected result is a proof leading to a desired statement (closed formula)
$\varphi$, that can be used as a certificate to the logical validity of
$\varphi$ --- that is, it can be checked mechanically to confirm the soundness
of each inference step.
While proof search can be a very costly process, due to the inherent complexity
of the search space, checking a proof is usually swift and done with linear
cost with respect to its size.

\paragraph{SMT Solvers} A somewhat newer approach based on extending the
Boolean satisfiability problem (SAT) with first-order theories, leading to
Satisfiability Modulo Theory (SMT).
SAT solvers accept Boolean formulas and construct a satisfying assignment, or
declare that none exists.
Since SAT is NP-complete (in fact, usually referred to as \emph{the} canonical
NP-complete problem), finding the solution may be time consuming, but eventual
termination is guaranteed.
SMT solvers take first-order formulas and attempt to find a first-order
\emph{model} for them, that is consistent with and underlying theory --- the
theory is built into the solver.
The theories generally put no bound on the size of the models they admit
(\eg numerical theories are typically defined over the domain of integers),
and as expected, the problem is generally undecidable.
However, despite the inherent complexity of SAT and undecidability of SMT,
solver technology as become extremely successful, especially in applications
relating to software verification.
In this context, proving a verification condition (``theorem'') $\varphi$
amounts to finding its negation $\lnot\varphi$ unsatisfiable --- which is the root
of the connection, and contention, between SMT solvers and theorem provers.
For many software verification conditions, SMT solvers have been shown to be
more effective in proving the validity of verification conditions than
contemporary proof theory-based theorem provers.

It is worth mentioning a third camp, that has, so far, received less
attention from the programming languages and automated reasoning crowd.

\paragraph{CSP Solvers} Constraint Satisfaction Problems (CSP) are ubiquitous
in planning, AI, and various other fields of computer science.
The ultimate goal of CSP is to find parameters that satisfy a set of (mostly
numerical) constraints. The solutions can be useful for scheduling tasks,
motion planning, \etc.
CSP solvers excel at solving puzzles such as the $N$-queen problem or Sudoku.
Their use in automated theorem proving is insofar limited.

\medskip
The primary goal of this research is to overcome the gap between the two camps,
one that currently prevents adoption of results from one approach into use in
the other.
We will strive to construct a unified framework for reasoning, where proving
and solving techniques can cooperate.
This will allow to leverage the power of SMT into proving the more complex
conjectures required for software verification and synthesis.
When it comes to applying SMT, one major obstacle in many problem domains, is
that SMT solvers are generally restricted to first-order logic; at the same time,
the properties one wishes to prove require the use of induction.
Theorem provers can more easily be equipped with induction-oriented inference
rules.
Still, sub-problems arising in the course of resolving the induction step can
and should benefit from SAT capabilities.

Another obstacle, which hinders both approaches, is the use of quantification
in assumptions and theorems.
One generally wishes to take advantage of the inherent modularity in proving
most kinds of properties, be those math theorems in algebra or combinatorics,
and definitely when it comes to correctness properties of computer programs.
% it is possible that the whole discussion of software modularity should be
% moved to the main body
Software is modular by nature, since modularity is what makes it possible
for human programmers to cope with its ever-growing complexity.
It is very much desirable to adopt the same contributing factor to reasoning about
these programs.
This means that instead of constructing one monolithic proof of the ``ultimate
theorem'', one identifies and proves \emph{lemmas} that abstract and generalize
domain knowledge and understanding of the underlying program --- layering them
up until the final goal is met.
This is definitely how it is done in academic papers, and, more recently, in
large software verification projects.
Going back to the challenge at hand, these auxiliary lemmas typically include
quantified formulas, which are logic's way of expressing generalized conjectures.
Applying these lemmas, \eg in the context of proving the next-step lemma,
ultimately requires to instantiate universal quantifiers with logical terms.
Since the space of available terms is large (or even infinite), this becomes
a task that is daunting to automate and requires the employment of heuristics
to select the right instances.
We will move to consolidate instantiation strategies into one that more closely
matches the search for proof --- thus being goal-directed rather then origin-directed.

As an ultimate artifact, this research will lay foundations for a new discipline
in automated provers, by way of integrating the camps of TP and SMT and allowing
cross-fertilization.
Specifically, it is motivated by provers for the tasks of software verification
and synthesis; these are characterized by relying almost exclusively on discrete
mathematics, such as set theory, graphs, automata, the construction of
natural numbers, and integer arithmetic.

\ldots


\section{Research Plan}

\subsection{Inductive-Deductive Reasoning Hybrid}

Deductive reasoning is the practice of applying general rules or statements to
individual, concrete cases; conversely, inductive reasoning is one of inferring,
from a set of observations, a unifying generalization.
Deductive reasoning can be thought of as a ``top-down'' application of logical
thinking, whereas inductive reasoning works ``bottom-up''.
While inferring a special case from a general case is always logically sound,
generalizing from examples can only guarantee soundness up to the set of
instances observed.
Still, even in the course of proving theorems, where soundness is of utmost
importance, humans habitually make use of inductive reasoning to guide their
deduction:
just a handful of examples can give rise to a useful generalization, a lemma,
which can then be proven deductively.

While there is no perfect analogue in the prover/solver setting as portrayed
in this document, it is quite obvious that theorem provers employ deductive
reasoning almost exclusively.
It would not be correct to say that SMT solvers employ inductive reasoning in
the same sense; a lot of their mechanics is still based on deduction.
However, state-of-the-art SMT solvers do make use of models generated in the
course of the search to guide deductive reasoning as this search progresses.
One notable example for this is Model-Based Quantifier Instantiation (MBQI),
a technique implemented in Z3.
In Sketch, a software synthesis tool, the system observes a set of inputs (and
outputs), proposes a generalization (a program), and then attempts to verify it
using a deductive method (in their case, reduction to SAT).
A growing number of examples is needed for the generalization to yield a correct
program, so this process repeats in a loop.

The idea of accumulating models of formulas or sub-formulas in the course of
a dialog between a deductive component and an inductive one can be lifted to
any type of reasoning.
When constructing proofs, existing models can be used to quickly vet possible
branching choices and discard ``obviously wrong'' ones: it is clearly futile
to try to prove a formula that is not valid, and any instance that does not
satisfy it can serve as a witness for that.
This is very much the process that a human mathematician may undergo when
considering which conjecture to prove next.
Clearly, having a representative set of models is essential for it, since
most intermediate statements have the form ``if $A_1$ and $A_2$ and $A_3$, then
$B$'', and models not satisfying one of the premises $A_i$ contribute no
relevant information.
It is thus desirable to identify ``pivot'' atomic formulas and keep around
models for various subsets of them.


\subsection{Unification Modulo Theories}

\ldots

\subsection{Feasible Induction Through Transitive Closure}

In the spirit of the mechanics of SMT solvers, where different \emph{theory solvers}
can be combined using methods such as DPLL(T), CDCL(T), and Nelson-Oppen, we
propose an important module would be a solver specifically capable of reasoning
about transitive closure properties.
Particularly, this solver will be able to construct models of quantifier-free
FO(TC) formulas.
FO(TC) is an extension of first-order logic with a new form of atomic formulas, $\big(\mathrm{TC}_{x,y}\varphi\big)(s,t)$,
whose semantics is the existence of a path $s=u_0,u_1,\ldots,u_n=t$, $n\geq 1$, with $\varphi[u_i/x,u_{i+1}/y]$
holding between all pairs of successive elements.
$x$ and $y$ are supposed free variables in $\varphi$, so $\varphi$ is commonly thought of as representing a binary relation, and $\big(\mathrm{TC}_{x,y}\varphi\big)$---the transitive closure of this relation.
A commonly used variant is $\big(\mathrm{RTC}_{x,y}\varphi\big)$, with the only difference being that $n\geq 0$ (instead of $1$),
which in turn induces the \emph{reflexive} transitive closure.
Another extension allows relations of arbitrary even arities
$\big(TC_{x_1,\ldots,x_k,y_1,\ldots,y_k}\varphi\big)$;
this transitive closure can be thought of as a path between $k$-tuples, where $x_1,\ldots,x_k$ and $y_1,\ldots,y_k$ represent coordinates of adjacent tuples.
In the sequel, we use the phrase ``transitive closure'' as an umbrella term for these closely related variants.

The proposal is to ingrain the notion of transitive closure in the language and proof search procedures of automated theorem provers.
The first expected benefit of this is that transitive closure enables clean, compact representation of data-related properties,
making the logic easier to use and the resulting formalisms clearer.
For example, the classical construction of the natural numbers from first principles involves a zero element and a successor operator.
\[
\begin{array}{ll}
\textrm{Inductive}~\mathrm{nat}~\eqdef \\
\quad |~0 : \tnat \\
\quad |~S : \tnat \to\tnat
\end{array}
\]

; this notion can be built into the solver
while constructing the model, as is the core property of transitivity, or,
more concretely, being a \emph{partial order}.
Solvers for the theory of equality (EUF) employ congruence closure as a core
algorithm; this can be generalized to partial orders and applied to this
context.


\subsubsection{Using Implicit Induction}

Most theorem provers, as well as some SMT solvers and systems using them,
prove inductive properties by the introduction of an \emph{induction scheme} ---
a specialized inference rule, whose premises comprise the induction base case
and step.
For example, given $P(0)$ and $\forall k.~ P(k)\rightarrow P(k+1)$,
one can infer $P(n)$ for any natural $n$.
In this way, the induction step effectively ``hides'' several application of
the induction hypothesis $P(k)$ inside a single inference step.
This is much different from the way humans approach the problem: they may set
out simplifying $P(k)$, realizing at some point that it may follow from $P(k-1)$,
justifying such reasoning based on the natural numbers being a wellfounded set.

We conjecture that this \emph{implicit} induction approach is suitable for
automatic proof discovery as well.
The formal implementation in this context is that of \emph{cyclic proof systems};
the cyclic framework admits proofs where some premises are satisfied by
conjectures occurring \emph{later} in the goal.
Of course, for such proofs to be sound some side conditions are imposed to assure
that a wellfounded order can be established.


\subsubsection{Overcome the Frame Problem with Frame Properties}

A prevalent difficulty of reasoning in logic is the \emph{frame problem}:
a constant need to specify not only effects we are interested to reason about,
but the entire environment surrounding it, which was not directly affected.
In the context of reasoning about computer systems, a program may effect a change
by storing or removing some data, causing a change in the system's state.
In order to prove properties of the system, and in particular, of such
accumulated changes, logic dictates that we formulate precisely what has
\emph{not} change in the system's state, including memory locations, database
tables, network communication packets \etc, that the program did not touch.
This is a huge hinderance to effective reasoning since properties that were
assumed or already proven with regard to the input state, have to be
re-stated and proven respective to the new state.
Such proofs are mundane and daunting, but there are many such properties
popping up in the course of a proof and they may very well require more work to
discharge than the ``insightful'' part.

\begin{figure}
\begin{lstlisting}
reverse(h) :=
  i := h; j := null;
  while (i != null) {
    t := i.next;
    i.next := j; j := i;
    i := t;
  }
\end{lstlisting}
\end{figure}

A classical example for the severity of the problem is analysis of the program
\textsf{reverse}, which reverses the order of nodes in a singly-linked list.
The program flips one edge at a time, and the effect of each iteration is
very local; however, to prove even a most basic property, that the program does
not introduce cycles in the list, the programmer has to include properties of
the prefix and suffix of the list, which have not changed --- the ``frame''.
The bookkeeping difficulty was so intense, that it led John C. Reynolds to the
conclusion that reasoning about pointer paths using transitive closure (heap
reachability) is not practical, and to found a new logic for dealing with
programs with dynamic heaps.
This logic is known as Separation Logic and is still the state-of-the-art in
reasoning about heaps.

%\begin{figure}
%\includegraphics[width=5cm]{standalone}
%\embedlatex[width=5cm]{standalone}
%\end{figure}

We claim that transitive closure \emph{can} be made effective by harnessing
\emph{frame properties} arising from the theory of transitive closure.
Frame properties are theorems and lemmas that assert the preservation of
certain properties, across consecutive program states, subject to the more
fine-grained properties preserved by the transition between them.
Frame properties can be generic or specific; \emph{generic} frame properties are
schemas that can apply to any $\varphi$ formula occurring within a transitive
closure $\big(\mathrm{TC}_{x,y}\varphi\big)$.
\emph{Specific} frame properties pertain to concrete programs and their respective
state transition semantics expressed as formulas.
An example of a generic property is: if there is a $\varphi$-path between some
two individuals $u, v$, and if for any $x,y$ $\varphi$-reachable from $u$, it
holds that $\varphi(x,y) \leftrightarrow \psi(x,y)$, then there is also a
$\psi$-path between $u, v$.
A derived, specific frame property could be the following: assume $n$ is a
function symbol representing the pointer links of a linked list, and assume
$n'$ is another function symbol representing the links of the same list after
a \emph{single iteration} of the program \textsf{reverse}.
Then all nodes reachable from the \emph{successor} of the loop iterator in the
former list, are still reachable from it in the second list.
This follows from the fact that the location of the modified pointer is not
reachable from any of the nodes in question, so the link paths in that area of
the list (the suffix) could not be affected.

By developing proof-support tools that ``guess'' specific frame properties
% xref lemma speculation
and prove them automatically based on more primitive facts and by applying
generic frame properties, we expect to greatly improve the efficacy of theorem
provers and also of interactive proof assistants in handling conjectures
involving transitive closure.
While frame properties do not, formally, play any special role compared to
other formulas occurring throughout the proof, they follow a unique kind of
``thought pattern'' that serves a mental purpose in abstracting away nitpicky
details.
This would allow us to focus human and machine attention alike on the more
nitty-gritty parts of the proof.

\subsubsection{Next-level Transitive Closure}

The use of transitive closure logic is not restricted to reasoning about heap
graphs, which mostly involved low-level kind of reasoning.
Many aspects of computing rely on iteration, \eg when expressing properties of
aggregate data --- a sum of a column in a database, for instance.
Indeed, there is a large and interesting space of programs that can be realized
with the combination of the functional operations \textsf{map}, \textsf{filter},
and \textsf{reduce}. % cite SPARK
Sets and sequences can be stored in many ways in memory and in non-volatile
storage, yet the iteration aspect remains the same and can be characterized by
a ``next-in-iteration'' function that matches for each element, the following
element in the iteration order.
The loopy construct, \eg sum, can then be axiomatized using the transitive
closure $\big(TC_{\langle a,u\rangle,\langle b,v\rangle}v=n(u) \land b=a+\mathrm{val}(u)\big)$
With the help of TC support in the theorem prover, and in the presence of a
library of theorems defining known properties of TC, reasoning about such
aggregates can be streamlined to be simpler than by using the corresponding
inductive definition of sum.
For example, distributivity of sum over sequence concatenation follows (almost)
for free from the transitivity of TC.



\subsection{Modular Reasoning with Lemmas}

Mathematical proofs are seldom monolithic.
Usually, there are some intermediate conjectures --- \emph{lemmas} --- that
are proved one by one, gradually building toward the culmination of the main
theorem.
It is not done out of altruism towards potential readers of the paper, though it can
certainly make a proof easier to understand.
Mainly, it helps the author overcome the complexity of the proof development
task: in a formal setting, even a seemingly benign logical step can have many
inference steps; and the steps are not always sequential, for example when
a case split is required in order to consider several scenarios.

This kind of modularization has been carried over by computer scientists to the
realm of interactive proof assistants.
In Type Theory, the use of lemmas is wired in as they are reduced to function
calls following the Curry-Howard Correspondence.
Modular proofs are even more of the essence in automated proof search, due to
the inherent computational complexity of the tool and poor scalability of
existing search techniques.
Breaking a proof task into lemmas also carries the promise of combining different
approaches to finding the proofs, by applying a portfolio approach in the spirit
of Why3 \cite{}.

\subsubsection{...}

\subsubsection{Near-match Lemma Speculation}

The following is a common scenario for anyone who has worked on formal proof development:
you have a conjecture to prove, and there is
a theorem whose conclusion seems to fit the conjecture, but does not \emph{quite} match.
As a very simple example, suppose the conjecture is
$\exists k.~ n = 2\cdot k$, and the preexisting theorem states:
\begin{equation}\label{manna-divmod}
  \forall i, j.~ i = \fdiv(i, j) \cdot j + \frem(i, j)
\end{equation}

(Inspired by Manna and Waldinger, 1980.)
Strict syntactic unification would fail to apply (\ref{manna-divmod}) to the proof goal $n = 2\cdot \evar{k}$, since (\ref{manna-divmod}) has a $+$ operator on the right-hand side of the equality.
(Here and in the sequel, we use $\evar{k}$ to designate a pattern variable that can be unified with anything.)
However, an intelligent mathematician knows that according to the basic laws of arithmetic, $\forall x.~x + 0 = x$.
It is possible to ``unblock'' the unification process if we equate $\frem(\cdot,\cdot) = 0$.
Similarly, unifying $\fdiv(n,\evar{j})\cdot \evar{j}$ with $2\cdot \evar{k}$ would fail, since $2$ cannot be unified with $\fdiv(n,\evar{j})$.
Again, by the law of commutativity, 
$\fdiv(n,\evar{j})\cdot j = \evar{j} \cdot\fdiv(n,\evar{j})$,
so we should be able to unify $\evar{j}$ with $2$ and $\fdiv(n, 2)$ with $\evar{k}$.

This process of \emph{unification modulo equalities}, or \emph{E-unification}, has been explored in the past but never exploited for automation\citeneeded.

{\color{gray}
transitive closure (see above) of a unary function $n$.
Transitivity dictates that this follows from $n^*(x,v), n^*(v,z)$ for some $v$,
and there is an assumption, or a previously proven conjecture, $n^*(x,y)$;
however, no assumption $n^*(y,z)$ is available.
Instead, what you have is $m^*(y,z)$ for a second function $m$.
This can be overcome by proving that $m^*(y,z) \Rightarrow n^*(y,z)$, as a
separate lemma.
To prove such a lemma, one may try to prove that $n=m$, or that $n^*=m^*$,
or that $\forall u, n^*(y,u)\leftrightarrow m^*(y,u)$, or any other generalization
of the missing implication.}

To mechanize the process, a prover is required to select a suitable version.
Some alternatives can be vetted out using the model-based approach described
earlier.
Proving the auxiliary lemma can also be postponed to a later stage, after the
prover has concluded the current proof branch and admitted it --- since a failed
proof branch will be discarded anyway, and all the lemmas used in it become
moot.
Hence the term ``speculation'', also used in works on proofs by rippling \cite{}.
In the original presentation, speculated lemmas are equalities between terms,
but they can be made far more general and integrate
into any kind of clause-based inference system such as natural deduction or
sequent calculus used by modern provers.


\subsubsection{Semi-eager Theory Propagation}

The T of SMT stands for \emph{Theory}.
A theory in this context comprises of a specific vocabulary, inducing a limited
subset of logical formulas, and a designated interpretation, thus also limiting
the logical \emph{structures} that can be considered for them.
One popular example, built into many SMT solvers, is the theory of integer linear
arithmetic (LIA): it provides the operators ``$+$'', ``$-$'', ``$\cdot$'' as
well as order relations ``$=$'', ``$\leq$''.
It imposes a syntactic restriction that two variables may not be multiplied;
a variable may only be multiplied by a constant numeric literal.
While a formula such as $5 < 3$ is \emph{logically} satisfiable, it is
unsatisfiable in the theory of LIA, since it requires all numeric literals to
be interpreted as the integers they represent and that ``$<$'' be interpreted
as the order of integers.

Communication between the SAT component and the theory component of the model
is handled by the theory solver producing \emph{learned clauses} that are valid
modulo the theory and a logically unsatisfiable when conjoined with the existing
goal.
For $5 < 3$, this may be as simple as $\lnot(5 < 3)$; but the clauses get more
involved as problem sizes and number of variables grow.
The generation of clauses by the theory solver is called
\emph{theory propagation}, and can be done in two ways: (i) \emph{eagerly}, by
observing the formula given to the solver and generating all the theory-valid
statements containing the terms it contains; and (ii) \emph{lazily}, waiting for
SAT to produce a Boolean assignment and then contradicting it with
appropriate clauses when it is inconsistent with the theory.

The eager approach has an inherent flaw: the number of such theory-valid
statements can grow very large very quickly.
It can be infinite, in the case of some theories, as most theories are
represented, conceptually speaking, by quantified formulas, and there is no
bound on the number of instantiations that may be needed to refute a single
statement.
Some theories, in particular those of integers, are not even definable by any
finite set of first-order formulas, quantified or otherwise.
Such difficulties cause eager propagation techniques to be all but abandoned.

The lazy approach, despite its popularity, can have severe run-time
implications.
It means that a number of SAT assignments have to be observed, and a
potentially computation intensive procedure consulted, to refute their
satisfiability in the logic.
Designers of SMT try to be clever in the way propagation clauses are generated,
since generating more general clauses will drive the SAT away from more
inconsistent assignments at an earlier stage, saving calls to theory solver.
This tactic is limited, however, since the theory solver only gets a glimpse
of a narrow case each time it is involved, and cannot make global decisions
based on the structure of the entire input problem.

We propose a middle ground, and that middle ground involves the use of proofs.
In a combined setting where SAT, theories, and formal proof objects work
together, intermediate proofs being explored provide ample subformulas to
fertilize propagation.
We will define an \emph{integration metric} for propagation clauses, that
quantitively expresses how tightly coupled a potential clause is with the
existing set of assumptions and goals present in the proof.
This can be thought of as a miniature ``page rank'' for logical formulas:
``hot'' atomic formulas, that is, ones that occur often in the proof, encourage
clauses that contain them, and these clauses, if accepted, will light up
more atomic formulas, with rank diminishing as they drift further from the
core.
Then, we will construct effective algorithms for finding such clauses that
optimize this metric.
We will tune the metric and the algorithms to achieve the fastest convergence
and compare to existing heuristics.

This semi-eager propagation of clauses from theories hold true potential for
a breakthrough with respect to handling theories and quantified conjectures
in unison, as challenged by Voronkov \cite{}.


\subsubsection{Lemma Patching}

Again, this is inspired by the way mathematicians work, first proving a
simplified version of a desired property as a lemma, then inspecting the proof
to figure out ``what would go wrong'' in the more general case --- using that
to prove a stronger lemma.
Intermediate proofs are thus raw material for more proofs: it lends a
``white-box'' view of lemmas, complementing the more basic use of lemmas in
their exact literal form, encapsulating their proofs (which can be seen as
``black-box'').
In a dual manner, occasionally a lemma is too strong: \eg if its conclusion is
a conjunction, but one is only interested in one of the conjuncts.
It is possible that a subset of the premises is sufficient to prove that one
conjunct.
Another example is when a conclusion is of the form $a < b$, but a relaxed set
of premises is enough to obtain $a \leq b$.

Moreover, during proof exploration itself (either by human or by machine), some
incorrect, dead-end proofs are inevitably encountered.
These are characterized, indeed identified, by assumptions that cannot be made
soundly (\eg a proof branch that includes formulas that are not logically valid).
Normally, a theorem prover simply discards these failed attempts and tries
another.
However, these too can be leveraged as raw material; repairing them amounts to
eliminating the spurious assumptions.
